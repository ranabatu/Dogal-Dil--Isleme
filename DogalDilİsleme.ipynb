{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba898d91-ee27-4d93-ab39-4c6b5fdd5635",
   "metadata": {},
   "source": [
    "# DOĞAL DİL İŞLEME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0818401-2246-406f-8fca-5a199cdc9c2d",
   "metadata": {},
   "source": [
    "### Stemming      \n",
    "Stemming, bir kelimenin kök formunu bulmak için kullanılan basit bir tekniktir.\n",
    "Kelimenin sonundaki ekleri keserek kelimenin köküne ulaşmayı hedefler. Örneğin, \"playing\", \"played\" ve \"plays\" kelimeleri \"play\" köküne indirgenir.\n",
    "Bu yöntem, genellikle kurallara dayalı olarak çalışır ve dil bilgisi kurallarını dikkate almaz. Bu yüzden bazen kelimenin gerçek anlamını bozan köklere ulaşabilir.\n",
    "Stemming sonucunda dil açısından mantıklı olmayan kökler elde edilebilir. Örneğin, \"connect\" ve \"connected\" kelimeleri \"connect\"e indirgenirken, \"connections\" kelimesi yanlış bir şekilde \"connect\" yerine \"connect\" köküne indirgenebilir.\n",
    "\n",
    "### Lemmatization   \n",
    "Lemmatization, kelimeleri sözlükte bulunan doğru biçimlerine (lemma) indirgemeyi amaçlayan daha gelişmiş bir tekniktir.\n",
    "Bu yöntem, kelimenin anlamını ve dil bilgisi kurallarını dikkate alır. Yani, kelimenin temel anlamını bozmadan doğru kök formuna ulaşmayı hedefler.\n",
    "Örneğin, \"running\", \"ran\" ve \"runs\" kelimeleri \"run\" lemmasına indirgenir. Ancak kelimenin bağlamını ve türünü dikkate alarak işlem yapar, bu nedenle kelimenin doğru kök halini sağlar.\n",
    "Lemmatization için genellikle bir sözlük ve dil bilgisi analizine ihtiyaç duyulur, bu da süreci daha karmaşık hale getirir, fakat sonuçlar daha doğru olur.\n",
    "\n",
    "Kısaca Farkları    \n",
    "Stemming: Daha basit ve hızlı bir işlemdir, ancak doğruluk oranı düşüktür.\n",
    "Lemmatization: Daha karmaşıktır ve daha doğru sonuçlar verir, çünkü dil bilgisi kurallarını ve kelimenin anlamını dikkate alır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be622b99-2d9b-4337-b413-6a275af458f0",
   "metadata": {},
   "source": [
    "## Stemmer Çeşitleri\n",
    "\n",
    "### Porter Stemmer   \n",
    "1979'da Martin Porter tarafından geliştirilmiş ve NLP'de en yaygın kullanılan stemmer'lardan biridir.\n",
    "İngilizce için oluşturulmuştur ve dil bilgisel kurallar kullanarak ekleri belirli kurallarla keser.\n",
    "Oldukça etkili ve yaygındır, ancak bazı durumlarda biraz yavaş çalışabilir.\n",
    "Porter Stemmer, kökleri doğru bir şekilde bulma konusunda oldukça başarılıdır, ancak bazen karmaşık kelimelerde hatalar yapabilir. Kelimenin sonundaki ekleri belirli bir sıraya göre keser ve kurallar içerir.\n",
    "Örneğin, \"caresses\" kelimesini \"caress\"e, \"ponies\" kelimesini \"poni\"ye indirger.\n",
    "\n",
    "### Snowball Stemmer   \n",
    "Snowball Stemmer, Martin Porter tarafından geliştirilen bir diğer stemmer'dır ve bazen \"Porter2\" olarak da bilinir.\n",
    "Daha esnek ve genişletilebilir bir yapıya sahiptir. Birçok dil için kullanılabilecek kurallar içerir ve bu nedenle farklı dillerde kullanılabilen bir çoklu dil desteği sunar.\n",
    "Daha güncel bir stemmer olup, Porter Stemmer'e göre daha tutarlı ve hızlı çalışır.\n",
    "Örneğin, İngilizce dışında Fransızca, Almanca, İspanyolca gibi diller için de destek sunar.\n",
    "\"Snowball\" ismi, aslında bu stemmer'ın geliştirildiği programlama dilinden gelmektedir ve algoritmanın kendisi de oldukça esnek yapılıdır.\n",
    "\n",
    "### Lancaster Stemmer  \n",
    "Lancaster Stemmer, aynı zamanda Paice-Husk Stemmer olarak da bilinir ve Porter Stemmer'dan daha agresif bir algoritmadır.\n",
    "Daha kısa ve hızlı sonuçlar verir, ancak bu da daha fazla hatalı sonuç anlamına gelebilir. Özellikle agresif yapısı nedeniyle, bazen kelimenin anlamını bozarak indirgeme yapabilir.\n",
    "Lancaster Stemmer, ekleri keserken oldukça serttir ve bu nedenle daha kısa kökler elde eder. Genellikle doğruluk oranı Porter ve Snowball Stemmer'a göre daha düşüktür.\n",
    "Örneğin, \"connection\" kelimesini \"connect\" yerine \"con\" olarak indirgemesi bu stemmer’ın agresif doğasını gösterir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6da5cc1-4ecd-4f9e-93d7-37d6d6aedc88",
   "metadata": {},
   "source": [
    "## Tokenization   \n",
    "Tokenization, metin işleme ve doğal dil işleme (NLP) alanında, bir metni daha küçük parçalara ayırma işlemidir. Bu parçalar genellikle kelimeler veya cümleler olabilir. Tokenization, metinlerin analiz edilmesi ve işlenmesi için önemli bir ilk adımdır, çünkü doğal dildeki metinler genellikle uzun ve karmaşıktır. Bu sayede metinler, makine öğrenimi modelleri veya diğer NLP işlemleri için daha kolay bir şekilde işlenebilir hale gelir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6727511a-a676-481f-ae17-9e2a42dfaf55",
   "metadata": {},
   "source": [
    "## Stopwords    \n",
    "Durdurma kelimeleri, metin işleme ve doğal dil işleme (NLP) alanlarında, genellikle çok sık kullanılan ve anlam taşıma değeri düşük olan kelimelerdir. Bu kelimeler, metinlerin temel anlamına katkı sağlamazlar ve metin analizlerinde çoğunlukla göz ardı edilirler. Stopwords listesi dil bazında değişir, ancak örneğin Türkçe'de \"ve\", \"bu\", \"bir\", \"ile\", \"ama\", \"gibi\" gibi kelimeler stopwords olarak kabul edilir.\n",
    "\n",
    "Stopwords'in Önemi ve Kullanımı  \n",
    "Gereksiz Kelimeleri Çıkarma: Stopwords'leri kaldırarak metindeki anlamsız kelimelerden kurtulmak, metin analizini daha verimli hale getirir.  \n",
    "İşlem Hızını Artırma: Anlamsız kelimelerin kaldırılması, veri boyutunu küçültür ve analiz sürecini hızlandırır.  \n",
    "Doğru Anlam Çıkarma: Önemli kelimeleri öne çıkararak metnin anlamını daha iyi kavramaya yardımcı olur.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bd90742-c66a-4b75-aaa6-e232d1ad09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edcf3601-3ce5-4400-acfe-6c1cc974f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingilizce stopwordleri yükleme\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a3382e9-7a8d-4a8b-b5eb-44170bc01bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The cats are RUNNING in the garden and playing with each other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3e22ea1-06d0-4141-be07-b45c4254a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b1d87a3e-0f5d-44e6-b0ed-92960ff23446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# küçük harfe dönüştürme ve noktalama işaretlerini kaldırma\n",
    "words = [word.lower() for word in words if word.isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fc7183bb-5a30-4ac1-9dd4-cd7c5c9d0d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwordsleri kaldırma\n",
    "words = [word for word in words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "122133a5-52da-4db5-8a10-fc3bd6f3ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming (kök çıkarma)\n",
    "porter_stemmer = PorterStemmer()\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3438cfa1-f573-47d3-921e-0679f5a494ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization (lematizasyon)\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [wordnet_lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06ff72b5-6206-4382-a7f5-c77aac763665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orjinal Cümle: The cats are RUNNING in the garden and playing with each other\n",
      "Tokenlar: ['cats', 'running', 'garden', 'playing']\n",
      "Stemmed Kelimeler: ['cat', 'run', 'garden', 'play']\n",
      "Lemmatized Cümle: ['cat', 'running', 'garden', 'playing']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Orjinal Cümle: {sentence}\")\n",
    "print(f\"Tokenlar: {words}\")\n",
    "print(f\"Stemmed Kelimeler: {stemmed_words}\")\n",
    "print(f\"Lemmatized Cümle: {lemmatized_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7b3f134-1e33-41a9-a85f-a317967403b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orjinal Cümle: Kediler bahçede koşuyor ve birbirleriyle oynuyorlar\n",
      "Tokenlar: ['kediler', 'bahçede', 'koşuyor', 'birbirleriyle', 'oynuyorlar']\n",
      "Stemmed Kelimeler: ['kedil', 'bahçed', 'koşuyor', 'birbirleriyl', 'oynuyorlar']\n",
      "Lemmatized Cümle: ['kediler', 'bahçede', 'koşuyor', 'birbirleriyle', 'oynuyorlar']\n"
     ]
    }
   ],
   "source": [
    "from snowballstemmer import TurkishStemmer\n",
    "\n",
    "stop_words = stopwords.words('turkish')\n",
    "\n",
    "cumle = \"Kediler bahçede koşuyor ve birbirleriyle oynuyorlar\"\n",
    "\n",
    "words = word_tokenize(cumle)\n",
    "\n",
    "words = [word.lower() for word in words if word.isalnum()]\n",
    "\n",
    "words = [word for word in words if word not in stop_words]\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(f\"Orjinal Cümle: {cumle}\")\n",
    "print(f\"Tokenlar: {words}\")\n",
    "print(f\"Stemmed Kelimeler: {stemmed_words}\")\n",
    "print(f\"Lemmatized Cümle: {lemmatized_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e902f4f-b74a-4ebf-aa59-eca8c372a814",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c09f9f7-ec13-4e14-a39d-f71c7904b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a08642f-3257-4e13-9264-9e4576dd5eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orjinal Sentences:\n",
      "- an example of a short ssentence\n",
      "- a second short sentence\n",
      "\n",
      "Processed Sentences (Stopwords Removed):\n",
      "- example short ssentence\n",
      "- second short sentence\n"
     ]
    }
   ],
   "source": [
    "# tokenizaiton ve stopwords'leri kaldırma fonksiyonu\n",
    "def removeStopWords(documents):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    result = []\n",
    "\n",
    "    for document in documents:\n",
    "        # Tokenization\n",
    "        words = word_tokenize(document)\n",
    "    \n",
    "        # Stopwordsleri kaldırma\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "        # Temizlenmiş cümleyi result listesine ekleme\n",
    "        result.append(\" \".join(filtered_words))\n",
    "        \n",
    "    return result\n",
    "\n",
    "#örnek cümleler\n",
    "cumleler = [\n",
    "        \"an example of a short sentence\",\n",
    "        \"a second short sentence\"\n",
    "]\n",
    "\n",
    "#tokenization ve stopwords'leri kaldırma işlemi\n",
    "yeni = removeStopWords(cumleler)\n",
    "\n",
    "#Sonucları yazdırma\n",
    "print(\"Orjinal Sentences:\")\n",
    "for cumle in cumleler:\n",
    "    print(f\"- {cumle}\")\n",
    "\n",
    "print(\"\\nProcessed Sentences (Stopwords Removed):\")\n",
    "for cumle in yeni:\n",
    "    print(f\"- {cumle}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396592b5-6990-4698-a19e-18afdeebfbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Belirtilen Kelimelerin Atılması, Belli Uzunluktaki Kelimelerin Atılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46447eb9-0e7c-4f62-ac09-fc0439e2201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75432e33-eec9-4964-b359-1059b86cd0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orjinal sentences:\n",
      "- an example of a short sentence\n",
      "- a second short sentence\n",
      "\n",
      "Uzun kelimelerin atılması:\n",
      "- an example of a short\n",
      "- a second short\n",
      "\n",
      "Kısa kelimelerin atılması:\n",
      "- an example of short sentence\n",
      "- second short sentence\n",
      "\n",
      "Belirtilen kelimelerin atılması:\n",
      "- an example of a sentence\n",
      "- a sentence\n"
     ]
    }
   ],
   "source": [
    "# Tokenization fonksiyonu\n",
    "def tokenizedDocument(documents):\n",
    "    result = []\n",
    "    for document in documents:\n",
    "        words = word_tokenize(document)\n",
    "        result.append(words)\n",
    "    return result\n",
    "\n",
    "# Uzun kelimeleri kaldıran Fonksiyon\n",
    "def removeLongWords(documents, max_length):\n",
    "    result = []\n",
    "    for document in documents:\n",
    "        filtered_words = [word for word in document if len(word) <= max_length]\n",
    "        result.append(filtered_words)\n",
    "    return result\n",
    "\n",
    "# Kısa kelimeleri kaldıran Fonksiyon\n",
    "def removeShortWords(documents, min_length):\n",
    "    result = []\n",
    "    for document in documents:\n",
    "        filtered_words = [word for word in document if len(word) >= min_length]\n",
    "        result.append(filtered_words)\n",
    "    return result\n",
    "\n",
    "# Belirli kelimeleri kaldıran fonksiyon\n",
    "def removeWords(documents, words_to_remove):\n",
    "    result = []\n",
    "    for document in documents:\n",
    "        filtered_words = [word for word in document if word not in words_to_remove]\n",
    "        result.append(filtered_words)\n",
    "    return result\n",
    "\n",
    "# Örnek cümleler\n",
    "cumleler = [\n",
    "    \"an example of a short sentence\",\n",
    "        \"a second short sentence\"\n",
    "]\n",
    "\n",
    "# Tokenization işlemi\n",
    "dokuman = tokenizedDocument(cumleler)\n",
    "\n",
    "# Uzun kelimeleri kaldırma işlemi\n",
    "uzun = removeLongWords(dokuman, 7)\n",
    "\n",
    "# Kısa kelimeleri kaldırma işlemi\n",
    "kisa = removeShortWords(dokuman, 2)\n",
    "\n",
    "# Belirli kelimeleri kaldırma işlemi\n",
    "kelimeler = [\"short\", \"second\"]\n",
    "istenilenkelimeatimi = removeWords(dokuman, kelimeler)\n",
    "\n",
    "#Sonucları yazdırma\n",
    "print(\"Orjinal sentences:\")\n",
    "for cumle in cumleler:\n",
    "    print(f\"- {cumle}\")\n",
    "\n",
    "print(\"\\nUzun kelimelerin atılması:\")\n",
    "for filtered_cumle in uzun:\n",
    "    print(f\"- {' '.join(filtered_cumle)}\")\n",
    "\n",
    "print(\"\\nKısa kelimelerin atılması:\")\n",
    "for filtered_cumle in kisa:\n",
    "    print(f\"- {' '.join(filtered_cumle)}\")\n",
    "\n",
    "print(\"\\nBelirtilen kelimelerin atılması:\")\n",
    "for filtered_cumle in istenilenkelimeatimi:\n",
    "    print(f\"- {' '.join(filtered_cumle)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d345ea-5b71-4f95-9a6e-1b83bd21b759",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "708fc61b-7256-4249-b118-3a4134b66a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is an exampler document.', 'It has two sentences.', 'This document has one sentence and an emotion.', ':) Here is another example document.', ':D']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "cumleler = [\"This is an exampler document. It has two sentences\",\n",
    "           \"This document has one sentence and an emotion. :)\",\n",
    "           \"Here is another example document. :D\"]\n",
    "\n",
    "tokenlar = [sent_tokenize(cumle) for cumle in cumleler]\n",
    "\n",
    "ifade = \"This is an exampler document. It has two sentences. This document has one sentence and an emotion. :) Here is another example document. :D\"\n",
    "print(sent_tokenize(ifade))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df45e3e6-b3a3-49e8-b6a7-7cfc284b89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,  word_tokenize\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "83219e0f-b41d-421e-821b-fd0b9d9148d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ranab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# önce nlk için gerekli kaynaklar indirilir\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f40cd6ab-175e-4f7e-add4-42e9d10676ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verilen metin\n",
    "cumleler = [\"This is an exampler document. It has two sentences\",\n",
    "           \"This document has one sentence and an emotion. :)\",\n",
    "           \"Here is another example document. :D\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "84acb736-3f89-4320-8b5e-a27d4b4cfc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════════════════╤═══════════════╤══════════════╤════════════════════╤════════════════════╤════════════╤═══════════════╤════════════════╤═══════════════╤══════════════════════╕\n",
      "│                │               │              │                    │                    │            │               │                │ Token         │ POS Tag              │\n",
      "╞════════════════╪═══════════════╪══════════════╪════════════════════╪════════════════════╪════════════╪═══════════════╪════════════════╪═══════════════╪══════════════════════╡\n",
      "│ This           │ is            │ an           │ exampler           │ document           │ .          │ It            │ has            │ two           │ sentences            │\n",
      "├────────────────┼───────────────┼──────────────┼────────────────────┼────────────────────┼────────────┼───────────────┼────────────────┼───────────────┼──────────────────────┤\n",
      "│ ('This', 'DT') │ ('is', 'VBZ') │ ('an', 'DT') │ ('exampler', 'NN') │ ('document', 'NN') │ ('.', '.') │ ('It', 'PRP') │ ('has', 'VBZ') │ ('two', 'CD') │ ('sentences', 'NNS') │\n",
      "╘════════════════╧═══════════════╧══════════════╧════════════════════╧════════════════════╧════════════╧═══════════════╧════════════════╧═══════════════╧══════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Cumleleri tokenleştir\n",
    "tokenlar = [word_tokenize(cumle) for cumle  in cumleler]\n",
    "\n",
    "# Token detayları al\n",
    "token_ayrinti = [(token, nltk.pos_tag(token)) for token in tokenlar]\n",
    "\n",
    "#token detaylarını ttablo olark göster\n",
    "print(tabulate(token_ayrinti[0], headers=['Token', 'POS Tag'], tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f94cb-ed9c-4ca5-9cbe-a3e0211b7f2b",
   "metadata": {},
   "source": [
    "## Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b7738cde-c6bd-4744-9486-60eec89a9e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Haydarpaşa', 'garında'], ['1941', 'baharında'], ['saat', 'on', 'beş', '.'], ['Merdivenlerin', 'üstünde', 'güneş'], ['yorgunluk', 've', 'telâş'], ['Bir', 'adam'], ['merdivenlerde', 'duruyor'], ['bir', 'şeyler', 'düşünerek', '.'], ['Zayıf', '.'], ['Korkak', '.'], ['Burnu', 'sivri', 've', 'uzun'], ['yanaklarının', 'üstü', 'çopur', '.'], ['Merdivenlerdeki', 'adam'], ['-', 'Galip', 'Usta', '-'], ['tuhaf', 'şeyler', 'düşünmekle'], ['meşhurdur', ':'], ['‘', '’', 'Kâat', 'helvası', 'yesem', 'her', 'gün', '‘', '’', 'diye', 'düşündü'], ['5', 'yaşında', '.'], ['‘', '’', 'Mektebe', 'gitsem', '‘', '’', 'diye', 'düşündü'], ['10', 'yaşında', '.'], ['‘', '’', 'Babamın', 'bıçakçı', 'dükkânından'], ['Akşam', 'ezanından', 'önce', 'çıksam', '‘', '’', 'diye', 'düşündü', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "dosya = \"siir.txt\"\n",
    "\n",
    "with open(dosya, 'r', encoding='utf-8') as file:\n",
    "    str_metin = file.read()\n",
    "\n",
    "#Metni satırlara böl\n",
    "textData = str_metin.split('\\n')\n",
    "\n",
    "# Cümleleri tokenleştir\n",
    "tokenler = [word_tokenize(cumle)for cumle in textData]\n",
    "\n",
    "print(tokenler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a820da-163c-4156-86aa-cc2fbe2f3827",
   "metadata": {},
   "source": [
    "## POS Taging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ac2e02c2-19ca-4f10-91f2-60f12584a6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════════════════╤════════════════════╤════════════════╤═══════════════╤════════════════════╤═══════════════╤══════════════╤═══════════════════╤════════════╤════════════╤════════════╕\n",
      "│                │                    │                │               │                    │               │              │                   │            │ Token      │ POS Tag    │\n",
      "╞════════════════╪════════════════════╪════════════════╪═══════════════╪════════════════════╪═══════════════╪══════════════╪═══════════════════╪════════════╪════════════╪════════════╡\n",
      "│ This           │ document           │ has            │ one           │ sentence           │ and           │ an           │ emotion           │ .          │ :          │ )          │\n",
      "├────────────────┼────────────────────┼────────────────┼───────────────┼────────────────────┼───────────────┼──────────────┼───────────────────┼────────────┼────────────┼────────────┤\n",
      "│ ('This', 'DT') │ ('document', 'NN') │ ('has', 'VBZ') │ ('one', 'CD') │ ('sentence', 'NN') │ ('and', 'CC') │ ('an', 'DT') │ ('emotion', 'NN') │ ('.', '.') │ (':', ':') │ (')', ')') │\n",
      "╘════════════════╧════════════════════╧════════════════╧═══════════════╧════════════════════╧═══════════════╧══════════════╧═══════════════════╧════════════╧════════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,  word_tokenize\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Verilen metin\n",
    "cumleler = [\"This is an exampler document. It has two sentences\",\n",
    "           \"This document has one sentence and an emotion. :)\",\n",
    "           \"Here is another example document. :D\"]\n",
    "\n",
    "# Cümleleri tokenleştir\n",
    "tokenler = [word_tokenize(cumle) for cumle in cumleler]\n",
    "\n",
    "# Token detayları için\n",
    "token_Ayrinti = [(token, nltk.pos_tag(token)) for token in tokenler]\n",
    "\n",
    "# Token detaylarını tablo olarak göster\n",
    "print(tabulate(token_ayrinti[1], headers=['Token', 'POS Tag'], tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d2b69-64d2-41e2-abfa-371f9fadf74f",
   "metadata": {},
   "source": [
    "## BAG OF WORDS (KELİME ÇANTASI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a6c635-dc19-4c23-a469-43074897ddc2",
   "metadata": {},
   "source": [
    "Bag of Words (BoW), doğal dil işleme (NLP) ve metin madenciliği alanlarında kullanılan basit ve popüler bir metin temsil yöntemidir. Bu yöntem, bir metindeki kelimelerin sıklığını dikkate alarak metni bir vektör olarak temsil eder, ancak kelimelerin dizilişini veya bağlamını dikkate almaz.\n",
    "\n",
    "Bag of Words Nasıl Çalışır?\n",
    "Kelime Sıklığı Hesaplama:\n",
    "\n",
    "İlk olarak, tüm metinlerde geçen benzersiz kelimelerden bir kelime listesi (sözlük) oluşturulur.\n",
    "Her bir metin, bu kelimelerin bulunduğu bir vektöre dönüştürülür.\n",
    "Vektör, her kelimenin kaç kez geçtiğini gösterir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3b184b83-b1bd-44b9-a416-141e90544ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelime Çantası Matrisi:\n",
      "[[0 1 0 1 0 0 0 1 1 0 0]\n",
      " [0 1 1 0 0 1 1 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0 1 1]]\n",
      "\n",
      "Kelimeler:\n",
      "['bugün' 'document' 'emotion' 'exampler' 'hava' 'one' 'sentence'\n",
      " 'sentences' 'two' 'yağışlı' 'çok']\n",
      " - This is an exampler document. It has two sentences\n",
      " - This document has one sentence and an emotion. \n",
      " - bugün hava çok yağışlı\n",
      "\n",
      "Tokenize Dökümanlr:\n",
      " - exampler document two sentences\n",
      " - document one sentence emotion\n",
      " - bugün hava çok yağışlı\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from  nltk.corpus import stopwords\n",
    "\n",
    "# Verilen metin\n",
    "documents = [\"This is an exampler document. It has two sentences\",\n",
    "           \"This document has one sentence and an emotion. \",\n",
    "           \"bugün hava çok yağışlı\"]\n",
    "\n",
    "#Tokenize ve Stopwords\n",
    "def tokenize_and_remove_stopwords(documents):\n",
    "    words = word_tokenize(documents)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    return words\n",
    "\n",
    "tokenized_documents = [' '.join(tokenize_and_remove_stopwords(doc)) for doc in documents]\n",
    "\n",
    "# CountVectorize ile kelime çantalarının oluşturulması\n",
    "vectorizer = CountVectorizer()\n",
    "bag_of_words = vectorizer.fit_transform(tokenized_documents)\n",
    "\n",
    "# kelime çantası matrisi\n",
    "bow_array = bag_of_words.toarray()\n",
    "\n",
    "# matrisin gösterimş\n",
    "print(\"Kelime Çantası Matrisi:\")\n",
    "print(bow_array)\n",
    "\n",
    "#kelimeler\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# kelimelerin gösterilmesi\n",
    "print(\"\\nKelimeler:\")\n",
    "print(feature_names)\n",
    "\n",
    "# Orjinal Dökümanlar\n",
    "for doc in documents:\n",
    "    print(f\" - {doc}\")\n",
    "\n",
    "# kelimelerin gösterilmesi\n",
    "print(\"\\nTokenize Dökümanlr:\")\n",
    "for doc in tokenized_documents:\n",
    "    print(f\" - {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f7fd2e-317b-425e-979f-b7522ba3d21f",
   "metadata": {},
   "source": [
    "## Elenmiş Kelime Çantası"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "22066947-dbb0-457c-b783-43c76873de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f5407a97-fc28-4c0e-89bd-89799790e074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Orijinal Dökümanlar:\n",
      " - kısa bir cümle örneği\n",
      " - ikinci kısa cümle\n",
      " - başka bir örnek\n",
      " - kısa bir örnek\n",
      "\n",
      "Tokenize Edilmiş Dökümanlar:\n",
      " - kısa bir cümle örneği\n",
      " - ikinci kısa cümle\n",
      " - başka bir örnek\n",
      " - kısa bir örnek\n",
      "\n",
      "Bag of Words Matrisi:\n",
      "[[0 1 1 0 1 0 1]\n",
      " [0 0 1 1 1 0 0]\n",
      " [1 1 0 0 0 1 0]\n",
      " [0 1 0 0 1 1 0]]\n",
      "\n",
      "Özellik (kelime) İndeksleri:\n",
      "['bir', 'cümle', 'kısa', 'örnek']\n",
      "\n",
      "Filtrelenmiş Bag Of Words Matrisi:\n",
      "[[1 1 1 0]\n",
      " [0 1 1 0]\n",
      " [1 0 0 1]\n",
      " [1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Tokenize ve Stopwords\n",
    "def tokenize_and_remove_stopwords(document):\n",
    "    words = word_tokenize(document)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    return words\n",
    "\n",
    "def tokenizeDocument(documents):\n",
    "    result = []\n",
    "    for document in documents:\n",
    "        words = tokenize_and_remove_stopwords(document)\n",
    "        result.append(' '.join(words))\n",
    "    return result\n",
    "\n",
    "def bagOfWords(documents):\n",
    "    vectorizer = CountVectorizer()\n",
    "    bag = vectorizer.fit_transform(documents)\n",
    "    return bag, vectorizer.get_feature_names_out()\n",
    "\n",
    "def removeInfrequentWords(bag, feature_names, count):\n",
    "    word_frequencies = np.array(bag.sum(axis=0))[0]\n",
    "    selected_word_indices = np.where(word_frequencies >= count)[0]\n",
    "    selected_words = [feature_names[i] for i in selected_word_indices]\n",
    "    filtered_bag = bag[:, selected_word_indices]\n",
    "    return filtered_bag, selected_words\n",
    "\n",
    "documents = [\n",
    "    \"kısa bir cümle örneği\",\n",
    "    \"ikinci kısa cümle\",\n",
    "    \"başka bir örnek\",\n",
    "    \"kısa bir örnek\"\n",
    "]\n",
    "\n",
    "# Tokenizasyon ve stopwords'leri kaldırma\n",
    "tokenized_documents = tokenizeDocument(documents)\n",
    "\n",
    "# Bag of words oluşturma\n",
    "bag, feature_names = bagOfWords(tokenized_documents)\n",
    "\n",
    "# Seyrek kelimeleri kaldırma\n",
    "count = 2\n",
    "newBag, selected_words = removeInfrequentWords(bag, feature_names, count)\n",
    "\n",
    "# kelimelerin gösterilmesi\n",
    "print(\"\\nOrijinal Dökümanlar:\")\n",
    "for doc in documents:\n",
    "    print(f\" - {doc}\")\n",
    "\n",
    "print(\"\\nTokenize Edilmiş Dökümanlar:\")\n",
    "for doc in tokenized_documents:\n",
    "    print(f\" - {doc}\")\n",
    "\n",
    "print(\"\\nBag of Words Matrisi:\")\n",
    "print(bag.toarray())\n",
    "\n",
    "print(\"\\nÖzellik (kelime) İndeksleri:\")\n",
    "print(selected_words)\n",
    "\n",
    "print(\"\\nFiltrelenmiş Bag Of Words Matrisi:\")\n",
    "print(newBag.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012618f-7007-46d4-9044-47a4ab302744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep]",
   "language": "python",
   "name": "conda-env-deep-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
